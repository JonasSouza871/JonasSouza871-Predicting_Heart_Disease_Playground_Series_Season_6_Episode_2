{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction - Modelos Simples\n",
    "\n",
    "Notebook dedicado a modelos clássicos de machine learning para a competição Kaggle Playground Series S6E2.\n",
    "Modelos simples servem como **baseline** e trazem **diversidade** para o ensemble final com os modelos boost.\n",
    "\n",
    "---\n",
    "\n",
    "## Modelos\n",
    "- Regressão Linear (Ridge)\n",
    "- Regressão Logística\n",
    "- Árvore de Decisão\n",
    "- Random Forest\n",
    "\n",
    "## Métrica: ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ENVIRONMENT SETUP\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Instalação de dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of all necessary dependencies\n",
    "!pip install -q scikit-learn pandas numpy matplotlib seaborn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_val_score\n",
    "from sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\n",
    "from sklearn.base import clone\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "RANDOM_STATE = 42 #Random seed para repordutibilidade\n",
    "np.random.seed(RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv(\"data/test.csv\") #carega dados de test\n",
    "train = pd.read_csv(\"data/train.csv\") #carrega dados de treino\n",
    "\n",
    "print(f\"Train shape: {train.shape}\")\n",
    "print(f\"Test shape: {test.shape}\")\n",
    "print(f\"\\nTotal records: {train.shape[0] + test.shape[0]:,}\") #quantidade todais de dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.head() #primeiras 5 linhas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature (Coluna) | Descrição | Tipo de Dado | Detalhes dos Valores |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Age** | Idade do paciente (em anos) | Numérico/Inteiro | Idade em anos. |\n",
    "| **Sex** | Gênero do paciente | Categórico/Binário | 1 = Masculino<br>0 = Feminino |\n",
    "| **Chest pain type** | Tipo de dor no peito | Categórico | 1 = Angina típica<br>2 = Angina atípica<br>3 = Dor não anginosa<br>4 = Assintomática |\n",
    "| **BP** | Pressão arterial em repouso (mm Hg) | Numérico/Inteiro | Valor da pressão arterial em mm Hg. |\n",
    "| **Cholesterol** | Nível de colesterol sérico (mg/dL) | Numérico/Inteiro | Valor do colesterol em mg/dL. |\n",
    "| **FBS over 120** | Glicemia em jejum > 120 mg/dL | Categórico/Binário | 1 = Verdadeiro<br>0 = Falso |\n",
    "| **EKG results** | Resultados de eletrocardiograma em repouso | Categórico | 0 = Normal<br>1 = Anormalidade da onda ST-T<br>2 = Hipertrofia ventricular esquerda |\n",
    "| **Max HR** | Frequência cardíaca máxima alcançada | Numérico/Inteiro | Batimentos máximos alcançados. |\n",
    "| **Exercise angina** | Angina induzida por exercício | Categórico/Binário | 1 = Sim<br>0 = Não |\n",
    "| **ST depression** | Depressão do ST induzida por exercício em relação ao repouso | Numérico/Decimal | Valor da depressão do segmento ST. |\n",
    "| **Slope of ST** | Inclinação do segmento ST do pico de exercício | Categórico | Descreve a inclinação do pico do exercício ST. |\n",
    "| **Number of vessels fluro** | Número de vasos principais (0-3) coloridos por fluoroscopia | Numérico/Inteiro | Quantidade de vasos (0 a 3). |\n",
    "| **Thallium** | Resultado do teste de estresse com Tálio (indicador médico categórico) | Categórico | Indicador médico categórico. |\n",
    "| **Heart Disease** | Variável alvo | Categórico (Alvo) | Presence = Doença cardíaca detectada<br>Absence = Sem doença cardíaca |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vemos a ausência de dados nulos, não necessitando de tratamento nessa etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.describe() #estatisticas básicas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separar os tipos de features e também o target(variavel alvo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Heart Disease\"\n",
    "id_col = \"id\"\n",
    "\n",
    "numeric_features = [\n",
    "    \"Age\",\n",
    "    \"BP\",\n",
    "    \"Cholesterol\",\n",
    "    \"Max HR\",\n",
    "    \"ST depression\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"Sex\",\n",
    "    \"Chest pain type\",\n",
    "    \"FBS over 120\",\n",
    "    \"EKG results\",\n",
    "    \"Exercise angina\",\n",
    "    \"Slope of ST\",\n",
    "    \"Number of vessels fluro\",\n",
    "    \"Thallium\"\n",
    "]\n",
    "\n",
    "print(f\"Target variable: '{target}'\")\n",
    "print(f\"ID column: '{id_col}'\")\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"\\nTotal features: {len(numeric_features) + len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = train.drop([id_col, target], axis=1) #x sendo os dados de treino sem o id (nao ajudara em nada no modelo e o target)\n",
    "y_train = train[target] #y sendo o target\n",
    "X_test = test.drop([id_col], axis=1) #x_teste sendo os dados sem ID, dados de teste não tem o targe (A nossa missão é descobrir)\n",
    "\n",
    "print(x_train.shape, y_train.shape, X_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Análise da variável Alvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
    "target_counts = y_train.value_counts() #contagem de dados target, devolverá a contagem de pessoas com doença e sem doença\n",
    "target_pct = y_train.value_counts(normalize=True) * 100 #normaliza a contagem onde a soma total é 100%\n",
    "print(target_pct)\n",
    "\n",
    "axes[0].bar(target_counts.index, target_counts.values, color=['#2ecc71', '#e74c3c'], edgecolor='black') #Grafico de barra com os valores absoluotos de contagem\n",
    "axes[0].set_title('Heart Disease Distribution (Counts)', fontsize=14, fontweight='bold')\n",
    "axes[0].set_xlabel('Heart Disease')\n",
    "axes[0].set_ylabel('Count')\n",
    "axes[0].set_xticks([0, 1])\n",
    "axes[0].set_xticklabels(['No Disease (0)', 'Disease (1)'])\n",
    "for i, v in enumerate(target_counts.values): #para aparecer o numero nas barras\n",
    "    axes[0].text(i, v + 1000, f'{v:,}', ha='center', fontweight='bold')\n",
    "\n",
    "axes[1].pie(target_counts.values, labels=['No Disease (0)', 'Disease (1)'], #grafico de pizza com os valores normalizados\n",
    "            autopct='%1.1f%%', startangle=90, colors=['#2ecc71', '#e74c3c'])\n",
    "axes[1].set_title('Heart Disease Distribution (%)', fontsize=14, fontweight='bold')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Essa proporção é ideal para o treinamento de modelos preditivos, pois a pequena diferença entre as classes minimiza o risco de viés e garante representatividade para ambos os grupos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Correlação entre features numéricas e target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data = x_train[numeric_features].copy()\n",
    "if y_train.dtype == 'object': #convert para dados numericas doneças ou sem doenças\n",
    "    y_numeric = y_train.map({'Absence': 0, 'Presence': 1}).values #convert 0 e 1\n",
    "    print(f\"Unique values: {y_train.unique()} -> {np.unique(y_numeric)}\")\n",
    "else:\n",
    "    y_numeric = y_train\n",
    "\n",
    "\n",
    "numeric_data['Heart Disease'] = y_numeric\n",
    "correlation_matrix = numeric_data.corr() #matriz de correlação\n",
    "target_corr = correlation_matrix['Heart Disease'].drop('Heart Disease').sort_values(ascending=False)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 6))\n",
    "sns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0, \n",
    "            ax=axes[0], linewidths=0.5, cbar_kws={'label': 'Correlation'})\n",
    "axes[0].set_title('Correlation Matrix (Numeric Features + Target)', fontsize=14, fontweight='bold')\n",
    "\n",
    "\n",
    "target_corr.plot(kind='barh', ax=axes[1], color=['green' if x > 0 else 'red' for x in target_corr]) #para saber quas influenciam em doenças\n",
    "axes[1].set_title('Feature Correlation with Heart Disease', fontsize=14, fontweight='bold')\n",
    "axes[1].set_xlabel('Correlation Coefficient')\n",
    "axes[1].axvline(0, color='black', linewidth=0.8)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Categorical Features vs Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_categorical = len(categorical_features) #quantidade de variaveis categorias\n",
    "n_cols = 3\n",
    "n_rows = (n_categorical + n_cols - 1) // n_cols #quantidade de linha para plots , 3 linhas de graficos\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4)) #subplot 3x3\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, feature in enumerate(categorical_features):\n",
    "    ax = axes[idx]\n",
    "    cross_tab = pd.crosstab(x_train[feature], y_train, normalize='index') * 100 #Cruza os dados para contar quantos pacientes existem em cada combinação\n",
    "    #Transforma os números brutos em proporções dentro de cada categoria. \n",
    "    cross_tab.plot(kind='bar', stacked=True, ax=ax, color=['#2ecc71', '#e74c3c'],  #plot das barras\n",
    "                   edgecolor='black', legend=False)\n",
    "    ax.set_title(f'{feature} vs Heart Disease', fontweight='bold')#titulo grafico\n",
    "    ax.set_xlabel(feature)\n",
    "    ax.set_ylabel('Percentage (%)')\n",
    "    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    ax.grid(alpha=0.3, axis='y')\n",
    "\n",
    "for idx in range(n_categorical, len(axes)):\n",
    "    axes[idx].axis('off')\n",
    "\n",
    "handles = [plt.Rectangle((0,0),1,1, color='#2ecc71'), plt.Rectangle((0,0),1,1, color='#e74c3c')]#cores legenda\n",
    "labels = ['No Disease (0)', 'Disease (1)'] #labels leganda\n",
    "fig.legend(handles, labels, loc='upper right', fontsize=12)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise das Variáveis Categóricas (Incidência de Doença)\n",
    "\n",
    "* **Sex:** Apresenta uma disparidade significativa; o grupo masculino (1) demonstra uma taxa de incidência de doença consideravelmente superior ao grupo feminino (0), sendo um forte divisor demográfico no modelo.\n",
    "* **Chest pain type:** O tipo 4 (Assintomática) é o indicador mais crítico, apresentando as maiores taxas de presença da doença, enquanto o tipo 2 (Angina atípica) apresenta a menor incidência proporcional.\n",
    "* **FBS over 120:** Embora indique risco metabólico, a taxa de doença entre quem tem glicemia alta (>120) é muito similar à de quem tem glicemia normal, sugerindo ser o preditor categórico de menor impacto individual.\n",
    "* **EKG results:** Pacientes com resultados tipo 2 (Hipertrofia ventricular esquerda) mostram uma probabilidade de doença cardíaca visivelmente maior em comparação aos que apresentam resultados normais (0).\n",
    "* **Exercise angina:** Um dos preditores mais binários; a presença de angina induzida por exercício (1) eleva drasticamente a taxa de doença cardíaca positiva em comparação a quem não manifesta o sintoma (0).\n",
    "* **Slope of ST:** A inclinação descendente ou plana no teste de esforço está fortemente correlacionada à presença da doença, enquanto a inclinação ascendente é um forte indicador de ausência (saúde cardíaca).\n",
    "* **Thallium:** Variável de alta especificidade; o resultado \"Reversível\" apresenta uma taxa de doença muito superior ao resultado \"Normal\", tornando-se um dos filtros mais precisos para o diagnóstico final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DATA PREPROCESSING\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vimos acima que somente o Target(y) necessita de transformação para numero, pois ele tem presence e absence e não valores númericos como 0 e 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Categorical Variables Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_transformed = y_train.map({'Absence': 0, 'Presence': 1}).values #absence tera valor 0 e presença valor\n",
    "print(f\"\\nTarget shape: {y_train_transformed.shape}\")\n",
    "print(f\"Unique values: {np.unique(y_train_transformed)}\")\n",
    "print(f\"Distribution: {np.bincount(y_train_transformed)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- vemos agora que so tem valores 0 e 1, ideal para o modelo que queremos trabalhar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "scaler_lr = StandardScaler()# Treinar regressão logística simples\n",
    "X_scaled = scaler_lr.fit_transform(x_train[numeric_features]) #normaliza dados numerricos\n",
    "lr = LogisticRegression(random_state=42)\n",
    "lr.fit(X_scaled, y_train_transformed) #treina modelo\n",
    "\n",
    "# Extrair coeficientes para usar na criaçao e features\n",
    "coefficients = dict(zip(numeric_features, lr.coef_[0]))\n",
    "print(\"Coeficientes aprendidos:\")\n",
    "for feat, coef in coefficients.items():\n",
    "    print(f\"  {feat}: {coef:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    eps = 1e-6\n",
    "    \n",
    "    # 1. Polynomial features (Square)\n",
    "    df[\"Age_sq\"] = df[\"Age\"] ** 2\n",
    "    df[\"Cholesterol_sq\"] = df[\"Cholesterol\"] ** 2\n",
    "    df[\"Max HR_sq\"] = df[\"Max HR\"] ** 2\n",
    "    df[\"ST depression_sq\"] = df[\"ST depression\"] ** 2\n",
    "    \n",
    "    # 2. Mathematical transformations (Log/Sqrt)\n",
    "    df[\"log_age\"] = np.log1p(df[\"Age\"])\n",
    "    df[\"log_cholesterol\"] = np.log1p(df[\"Cholesterol\"])\n",
    "    df[\"log_st_depression\"] = np.log1p(df[\"ST depression\"])\n",
    "    df[\"sqrt_age\"] = np.sqrt(df[\"Age\"])\n",
    "    df[\"sqrt_cholesterol\"] = np.sqrt(df[\"Cholesterol\"])\n",
    "    df[\"sqrt_maxhr\"] = np.sqrt(df[\"Max HR\"])\n",
    "    \n",
    "    # 3. Interactions (Multiplications)\n",
    "    df[\"age_x_st_depression\"] = df[\"Age\"] * df[\"ST depression\"]\n",
    "    df[\"maxhr_x_st_depression\"] = df[\"Max HR\"] * df[\"ST depression\"]\n",
    "    df[\"age_x_maxhr\"] = df[\"Age\"] * df[\"Max HR\"]\n",
    "    df[\"age_x_cholesterol\"] = df[\"Age\"] * df[\"Cholesterol\"]\n",
    "    df[\"cholesterol_x_st_depression\"] = df[\"Cholesterol\"] * df[\"ST depression\"]\n",
    "\n",
    "    # 4. Ratios (Divisions)\n",
    "    df[\"cholesterol_per_age\"] = df[\"Cholesterol\"] / (df[\"Age\"] + eps)\n",
    "    df[\"maxhr_per_age\"] = df[\"Max HR\"] / (df[\"Age\"] + eps)\n",
    " \n",
    "    # 5. Differences (Deviation from mean)\n",
    "    df[\"age_diff_mean\"] = df[\"Age\"] - df[\"Age\"].mean()\n",
    "    df[\"cholesterol_diff_mean\"] = df[\"Cholesterol\"] - df[\"Cholesterol\"].mean()\n",
    "    df[\"maxhr_diff_mean\"] = df[\"Max HR\"] - df[\"Max HR\"].mean()\n",
    "    \n",
    "    # 6. Binning (Discretization)\n",
    "    df[\"age_bin\"] = pd.cut(df[\"Age\"], bins=[-1, 40, 50, 60, 100], labels=False)\n",
    "    df[\"cholesterol_bin\"] = pd.cut(df[\"Cholesterol\"], bins=[-1, 200, 240, 280, 600], labels=False)\n",
    "    df[\"maxhr_bin\"] = pd.cut(df[\"Max HR\"], bins=[-1, 100, 130, 160, 250], labels=False)\n",
    "    df[\"st_bin\"] = pd.cut(df[\"ST depression\"], bins=[-1, 0.5, 1.5, 3.0, 10], labels=False)\n",
    "    \n",
    "    # 7. Boolean Flags (Risk factors)\n",
    "    df[\"elderly\"] = (df[\"Age\"] >= 60).astype(int)\n",
    "    df[\"high_cholesterol\"] = (df[\"Cholesterol\"] >= 240).astype(int)\n",
    "    df[\"high_st_depression\"] = (df[\"ST depression\"] > 2.0).astype(int)\n",
    "    \n",
    "    # 8. Risk Scores\n",
    "    df[\"learned_risk_score\"] = (\n",
    "        df[\"Age\"] * 0.4244 +\n",
    "        df[\"BP\"] * (-0.0044) +\n",
    "        df[\"Cholesterol\"] * 0.1441 +\n",
    "        df[\"Max HR\"] * (-1.0234) +\n",
    "        df[\"ST depression\"] * 0.9945\n",
    "    )\n",
    "    \n",
    "    df[\"risk_factors_count\"] = (\n",
    "        df[\"elderly\"] + \n",
    "        df[\"high_cholesterol\"] + \n",
    "        df[\"high_st_depression\"]\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = create_features(x_train)\n",
    "X_test_features  = create_features(X_test)\n",
    "print(f\"Features após create_features: {x_train_features.shape[1]}\")\n",
    "\n",
    "# --- 9. Frequency Encoding ---\n",
    "all_data = pd.concat([x_train_features, X_test_features])\n",
    "all_features = numeric_features + categorical_features\n",
    "for col in all_features:\n",
    "    freq = all_data[col].value_counts(normalize=True)\n",
    "    x_train_features[col + \"_freq\"] = x_train_features[col].map(freq)\n",
    "    X_test_features[col + \"_freq\"] = X_test_features[col].map(freq)\n",
    "print(f\"Features após Frequency Encoding: {x_train_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Feature Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation with target\n",
    "df_corr = x_train_features.copy()\n",
    "df_corr['target'] = y_train_transformed\n",
    "corr_target = df_corr.corr(numeric_only=True)['target'].drop('target') #calcula correlação com o target\n",
    "corr_sorted = corr_target.abs().sort_values(ascending=False)\n",
    "\n",
    "#Visualization Features\n",
    "height = 12\n",
    "plt.figure(figsize=(14, height))\n",
    "\n",
    "top_corr = corr_target.loc[corr_sorted.head(42).index]\n",
    "colors = ['green' if x > 0 else 'red' for x in top_corr.values]\n",
    "\n",
    "bars = plt.barh(range(len(top_corr)), top_corr.values, color=colors)\n",
    "plt.yticks(range(len(top_corr)), top_corr.index)\n",
    "plt.xlabel('Correlation Coefficient', fontsize=12)\n",
    "plt.title('Top Features - Correlation with Heart Disease', fontsize=14, fontweight='bold')\n",
    "plt.axvline(0, color='black', linewidth=0.8)\n",
    "plt.grid(alpha=0.3, axis='x')\n",
    "\n",
    "for i, (feat, val) in enumerate(zip(top_corr.index, top_corr.values)):\n",
    "    plt.text(val + 0.01 if val > 0 else val - 0.01, i, f'{val:.3f}', \n",
    "             va='center', ha='left' if val > 0 else 'right', fontsize=9)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Model Evaluation Function (Cross-Validation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.base import clone\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model_cv(model, X, y, X_test=None, cv=5):\n",
    "    \"\"\"\n",
    "    Avalia modelo com cross-validation e retorna:\n",
    "    - roc_scores: array com ROC-AUC de cada fold\n",
    "    - oof_preds: predições out-of-fold no treino\n",
    "    - test_preds: predições no teste (média dos folds) - só se X_test for passado\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    roc_auc_scores = []\n",
    "    oof_preds = np.zeros(len(y))\n",
    "    \n",
    "    if X_test is not None:\n",
    "        test_preds_folds = np.zeros((len(X_test), cv))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        if hasattr(X, 'iloc'):\n",
    "            X_train_fold = X.iloc[train_idx]\n",
    "            X_val_fold = X.iloc[val_idx]\n",
    "        else:\n",
    "            X_train_fold = X[train_idx]\n",
    "            X_val_fold = X[val_idx]\n",
    "        \n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        model_fold = clone(model)\n",
    "        \n",
    "        model_fold.fit(X_train_fold, y_train_fold)\n",
    "        oof_preds[val_idx] = model_fold.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        if X_test is not None:\n",
    "            test_preds_folds[:, fold] = model_fold.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_val_fold, oof_preds[val_idx])\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "    \n",
    "    if X_test is not None:\n",
    "        test_preds = test_preds_folds.mean(axis=1)\n",
    "        return np.array(roc_auc_scores), oof_preds, test_preds\n",
    "    \n",
    "    return np.array(roc_auc_scores), oof_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Normalização (StandardScaler)\n",
    "\n",
    "Regressão Linear e Logística são sensíveis à escala dos dados. Normalizamos para que todas as features contribuam igualmente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(x_train_features)\n",
    "X_test_scaled = scaler.transform(X_test_features)\n",
    "\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Regressão Linear (Ridge)\n",
    "\n",
    "----------\n",
    "\n",
    "Usa regressão linear com regularização L2 (Ridge) para prever probabilidades. Como é um problema de classificação, fazemos clip das predições entre 0 e 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "\n",
    "# Regressão Linear não tem predict_proba, então fazemos CV manual\n",
    "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "roc_auc_scores_ridge = []\n",
    "oof_ridge = np.zeros(len(y_train_transformed))\n",
    "test_preds_ridge = np.zeros((len(X_test_scaled), 5))\n",
    "\n",
    "for fold, (train_idx, val_idx) in enumerate(skf.split(X_train_scaled, y_train_transformed)):\n",
    "    X_tr = X_train_scaled[train_idx]\n",
    "    X_val = X_train_scaled[val_idx]\n",
    "    y_tr = y_train_transformed[train_idx]\n",
    "    y_val = y_train_transformed[val_idx]\n",
    "    \n",
    "    ridge = Ridge(alpha=1.0, random_state=42)\n",
    "    ridge.fit(X_tr, y_tr)\n",
    "    \n",
    "    # Clip entre 0 e 1 para simular probabilidades\n",
    "    oof_ridge[val_idx] = np.clip(ridge.predict(X_val), 0, 1)\n",
    "    test_preds_ridge[:, fold] = np.clip(ridge.predict(X_test_scaled), 0, 1)\n",
    "    \n",
    "    roc_auc = roc_auc_score(y_val, oof_ridge[val_idx])\n",
    "    roc_auc_scores_ridge.append(roc_auc)\n",
    "\n",
    "y_test_proba_ridge = test_preds_ridge.mean(axis=1)\n",
    "\n",
    "for fold, score in enumerate(roc_auc_scores_ridge, 1):\n",
    "    print(f\"  Fold {fold}: {score:.4f}\")\n",
    "print(f\"\\nMean ROC-AUC: {np.mean(roc_auc_scores_ridge):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_ridge = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Heart Disease': y_test_proba_ridge\n",
    "})\n",
    "\n",
    "submission_ridge.to_csv('submission_Ridge.csv', index=False)\n",
    "print(\"Ridge submission salvo: submission_Ridge.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Regressão Logística\n",
    "\n",
    "----------\n",
    "\n",
    "Modelo linear clássico para classificação binária. Usa dados normalizados para melhor convergência."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "logreg_model = LogisticRegression(\n",
    "    C=1.0,\n",
    "    max_iter=1000,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "# Usa dados normalizados\n",
    "roc_scores_lr, oof_logreg, y_test_proba_logreg = evaluate_model_cv(\n",
    "    logreg_model, X_train_scaled, y_train_transformed, X_test_scaled, cv=5\n",
    ")\n",
    "\n",
    "for fold, score in enumerate(roc_scores_lr, 1):\n",
    "    print(f\"  Fold {fold}: {score:.4f}\")\n",
    "print(f\"\\nMean ROC-AUC: {roc_scores_lr.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_logreg = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Heart Disease': y_test_proba_logreg\n",
    "})\n",
    "\n",
    "submission_logreg.to_csv('submission_LogReg.csv', index=False)\n",
    "print(\"Logistic Regression submission salvo: submission_LogReg.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Árvore de Decisão\n",
    "\n",
    "----------\n",
    "\n",
    "Modelo baseado em regras de decisão. Não precisa de normalização. Serve como baseline para comparar com Random Forest."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "dt_model = DecisionTreeClassifier(\n",
    "    max_depth=10,\n",
    "    min_samples_split=20,\n",
    "    min_samples_leaf=10,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Árvore não precisa de normalização, usa features originais\n",
    "roc_scores_dt, oof_dt, y_test_proba_dt = evaluate_model_cv(\n",
    "    dt_model, x_train_features, y_train_transformed, X_test_features, cv=5\n",
    ")\n",
    "\n",
    "for fold, score in enumerate(roc_scores_dt, 1):\n",
    "    print(f\"  Fold {fold}: {score:.4f}\")\n",
    "print(f\"\\nMean ROC-AUC: {roc_scores_dt.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_dt = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Heart Disease': y_test_proba_dt\n",
    "})\n",
    "\n",
    "submission_dt.to_csv('submission_DecisionTree.csv', index=False)\n",
    "print(\"Decision Tree submission salvo: submission_DecisionTree.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.4 Random Forest\n",
    "\n",
    "----------\n",
    "\n",
    "Ensemble de árvores de decisão. Combina múltiplas árvores para reduzir variância e melhorar generalização."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=200,\n",
    "    max_depth=10,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "\n",
    "roc_scores_rf, oof_rf, y_test_proba_rf = evaluate_model_cv(\n",
    "    rf_model, x_train_features, y_train_transformed, X_test_features, cv=5\n",
    ")\n",
    "\n",
    "for fold, score in enumerate(roc_scores_rf, 1):\n",
    "    print(f\"  Fold {fold}: {score:.4f}\")\n",
    "print(f\"\\nMean ROC-AUC: {roc_scores_rf.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_rf = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Heart Disease': y_test_proba_rf\n",
    "})\n",
    "\n",
    "submission_rf.to_csv('submission_RF.csv', index=False)\n",
    "print(\"Random Forest submission salvo: submission_RF.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.5 Comparação dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Resumo de todos os modelos\n",
    "models_summary = {\n",
    "    'Ridge': np.mean(roc_auc_scores_ridge),\n",
    "    'Logistic Regression': roc_scores_lr.mean(),\n",
    "    'Decision Tree': roc_scores_dt.mean(),\n",
    "    'Random Forest': roc_scores_rf.mean(),\n",
    "}\n",
    "\n",
    "print(\"=\" * 45)\n",
    "print(f\"{'Modelo':<25} {'ROC-AUC':>10}\")\n",
    "print(\"=\" * 45)\n",
    "for model, score in sorted(models_summary.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(f\"{model:<25} {score:>10.4f}\")\n",
    "print(\"=\" * 45)\n",
    "\n",
    "# Gráfico comparativo\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "names = list(models_summary.keys())\n",
    "scores = list(models_summary.values())\n",
    "colors = ['#3498db', '#2ecc71', '#e67e22', '#e74c3c']\n",
    "bars = ax.bar(names, scores, color=colors, edgecolor='black')\n",
    "ax.set_ylabel('ROC-AUC')\n",
    "ax.set_title('Comparação dos Modelos Simples', fontweight='bold')\n",
    "ax.set_ylim(min(scores) - 0.02, max(scores) + 0.01)\n",
    "for bar, score in zip(bars, scores):\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.002,\n",
    "            f'{score:.4f}', ha='center', fontweight='bold')\n",
    "ax.grid(alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.6 Ensemble Simples\n",
    "\n",
    "Média simples das probabilidades dos 4 modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensemble Simples - média das probabilidades dos 4 modelos\n",
    "y_ensemble_simple = (y_test_proba_ridge + y_test_proba_logreg + y_test_proba_dt + y_test_proba_rf) / 4\n",
    "\n",
    "submission_ensemble_simple = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Heart Disease': y_ensemble_simple\n",
    "})\n",
    "\n",
    "submission_ensemble_simple.to_csv('submission_ensemble_simples.csv', index=False)\n",
    "print(\"Ensemble Simples salvo: submission_ensemble_simples.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.7 Ensemble Ponderado\n",
    "\n",
    "Média ponderada das probabilidades, com pesos proporcionais ao score OOF de cada modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scores calculados automaticamente a partir do OOF de cada modelo\n",
    "scores = {\n",
    "    'ridge': roc_auc_score(y_train_transformed, oof_ridge),\n",
    "    'logreg': roc_auc_score(y_train_transformed, oof_logreg),\n",
    "    'dt': roc_auc_score(y_train_transformed, oof_dt),\n",
    "    'rf': roc_auc_score(y_train_transformed, oof_rf)\n",
    "}\n",
    "\n",
    "# Pesos normalizados proporcionais ao score\n",
    "total = sum(scores.values())\n",
    "w_ridge = scores['ridge'] / total\n",
    "w_logreg = scores['logreg'] / total\n",
    "w_dt = scores['dt'] / total\n",
    "w_rf = scores['rf'] / total\n",
    "\n",
    "print(f\"Scores OOF:\")\n",
    "print(f\"  Ridge:              {scores['ridge']:.6f} (peso: {w_ridge:.4f})\")\n",
    "print(f\"  Logistic Regression:{scores['logreg']:.6f} (peso: {w_logreg:.4f})\")\n",
    "print(f\"  Decision Tree:      {scores['dt']:.6f} (peso: {w_dt:.4f})\")\n",
    "print(f\"  Random Forest:      {scores['rf']:.6f} (peso: {w_rf:.4f})\")\n",
    "\n",
    "# Ensemble Ponderado\n",
    "y_ensemble_weighted = (w_ridge * y_test_proba_ridge) + (w_logreg * y_test_proba_logreg) + \\\n",
    "                      (w_dt * y_test_proba_dt) + (w_rf * y_test_proba_rf)\n",
    "\n",
    "submission_ensemble_weighted = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Heart Disease': y_ensemble_weighted\n",
    "})\n",
    "\n",
    "submission_ensemble_weighted.to_csv('submission_ensemble_simples_weighted.csv', index=False)\n",
    "print(\"\\nEnsemble ponderado salvo: submission_ensemble_simples_weighted.csv\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}