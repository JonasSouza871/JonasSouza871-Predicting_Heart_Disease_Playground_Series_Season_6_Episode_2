{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction - MLP (Multi-Layer Perceptron)\n",
    "\n",
    "Notebook dedicado ao modelo **MLP (Rede Neural)** para a competição Kaggle Playground Series S6E2.\n",
    "O MLP aprende fronteiras de decisão de forma diferente dos modelos baseados em árvore (XGBoost, LightGBM, CatBoost), trazendo **diversidade** para o ensemble final.\n",
    "\n",
    "---\n",
    "\n",
    "## Métrica: ROC-AUC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. ENVIRONMENT SETUP\n",
    "\n",
    "-------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 Instalação de dependências"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Installation of all necessary dependencies\n",
    "!pip install -q optuna scikit-learn pandas numpy matplotlib seaborn tqdm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "import pandas as pd\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.model_selection import KFold, StratifiedKFold, train_test_split, cross_val_score\nfrom sklearn.metrics import roc_auc_score, roc_curve, confusion_matrix, classification_report\nfrom sklearn.base import clone\nfrom sklearn.neural_network import MLPClassifier\nfrom sklearn.preprocessing import StandardScaler\nimport optuna\nfrom optuna.samplers import TPESampler\nimport warnings\nwarnings.filterwarnings('ignore')\n\nRANDOM_STATE = 42 #Random seed para repordutibilidade\nnp.random.seed(RANDOM_STATE)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 Data Loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "test = pd.read_csv(\"data/test.csv\") #carega dados de test\ntrain = pd.read_csv(\"data/train.csv\") #carrega dados de treino\n\nprint(f\"Train shape: {train.shape}\")\nprint(f\"Test shape: {test.shape}\")\nprint(f\"\\nTotal records: {train.shape[0] + test.shape[0]:,}\") #quantidade todais de dados"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train.head() #primeiras 5 linhas"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Feature (Coluna) | Descrição | Tipo de Dado | Detalhes dos Valores |\n",
    "| :--- | :--- | :--- | :--- |\n",
    "| **Age** | Idade do paciente (em anos) | Numérico/Inteiro | Idade em anos. |\n",
    "| **Sex** | Gênero do paciente | Categórico/Binário | 1 = Masculino<br>0 = Feminino |\n",
    "| **Chest pain type** | Tipo de dor no peito | Categórico | 1 = Angina típica<br>2 = Angina atípica<br>3 = Dor não anginosa<br>4 = Assintomática |\n",
    "| **BP** | Pressão arterial em repouso (mm Hg) | Numérico/Inteiro | Valor da pressão arterial em mm Hg. |\n",
    "| **Cholesterol** | Nível de colesterol sérico (mg/dL) | Numérico/Inteiro | Valor do colesterol em mg/dL. |\n",
    "| **FBS over 120** | Glicemia em jejum > 120 mg/dL | Categórico/Binário | 1 = Verdadeiro<br>0 = Falso |\n",
    "| **EKG results** | Resultados de eletrocardiograma em repouso | Categórico | 0 = Normal<br>1 = Anormalidade da onda ST-T<br>2 = Hipertrofia ventricular esquerda |\n",
    "| **Max HR** | Frequência cardíaca máxima alcançada | Numérico/Inteiro | Batimentos máximos alcançados. |\n",
    "| **Exercise angina** | Angina induzida por exercício | Categórico/Binário | 1 = Sim<br>0 = Não |\n",
    "| **ST depression** | Depressão do ST induzida por exercício em relação ao repouso | Numérico/Decimal | Valor da depressão do segmento ST. |\n",
    "| **Slope of ST** | Inclinação do segmento ST do pico de exercício | Categórico | Descreve a inclinação do pico do exercício ST. |\n",
    "| **Number of vessels fluro** | Número de vasos principais (0-3) coloridos por fluoroscopia | Numérico/Inteiro | Quantidade de vasos (0 a 3). |\n",
    "| **Thallium** | Resultado do teste de estresse com Tálio (indicador médico categórico) | Categórico | Indicador médico categórico. |\n",
    "| **Heart Disease** | Variável alvo | Categórico (Alvo) | Presence = Doença cardíaca detectada<br>Absence = Sem doença cardíaca |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test.isnull().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vemos a ausência de dados nulos, não necessitando de tratamento nessa etapa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "train.describe() #estatisticas básicas"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Separar os tipos de features e também o target(variavel alvo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target = \"Heart Disease\"\n",
    "id_col = \"id\"\n",
    "\n",
    "numeric_features = [\n",
    "    \"Age\",\n",
    "    \"BP\",\n",
    "    \"Cholesterol\",\n",
    "    \"Max HR\",\n",
    "    \"ST depression\"\n",
    "]\n",
    "\n",
    "categorical_features = [\n",
    "    \"Sex\",\n",
    "    \"Chest pain type\",\n",
    "    \"FBS over 120\",\n",
    "    \"EKG results\",\n",
    "    \"Exercise angina\",\n",
    "    \"Slope of ST\",\n",
    "    \"Number of vessels fluro\",\n",
    "    \"Thallium\"\n",
    "]\n",
    "\n",
    "print(f\"Target variable: '{target}'\")\n",
    "print(f\"ID column: '{id_col}'\")\n",
    "print(f\"\\nNumeric features ({len(numeric_features)}): {numeric_features}\")\n",
    "print(f\"\\nCategorical features ({len(categorical_features)}): {categorical_features}\")\n",
    "print(f\"\\nTotal features: {len(numeric_features) + len(categorical_features)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "x_train = train.drop([id_col, target], axis=1) #x sendo os dados de treino sem o id (nao ajudara em nada no modelo e o target)\ny_train = train[target] #y sendo o target\nX_test = test.drop([id_col], axis=1) #x_teste sendo os dados sem ID, dados de teste não tem o targe (A nossa missão é descobrir)\n\nprint(x_train.shape, y_train.shape, X_test.shape)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. EDA\n",
    "\n",
    "------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1 Análise da variável Alvo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\ntarget_counts = y_train.value_counts() #contagem de dados target, devolverá a contagem de pessoas com doença e sem doença\ntarget_pct = y_train.value_counts(normalize=True) * 100 #normaliza a contagem onde a soma total é 100%\nprint(target_pct)\n\naxes[0].bar(target_counts.index, target_counts.values, color=['#2ecc71', '#e74c3c'], edgecolor='black') #Grafico de barra com os valores absoluotos de contagem\naxes[0].set_title('Heart Disease Distribution (Counts)', fontsize=14, fontweight='bold')\naxes[0].set_xlabel('Heart Disease')\naxes[0].set_ylabel('Count')\naxes[0].set_xticks([0, 1])\naxes[0].set_xticklabels(['No Disease (0)', 'Disease (1)'])\nfor i, v in enumerate(target_counts.values): #para aparecer o numero nas barras\n    axes[0].text(i, v + 1000, f'{v:,}', ha='center', fontweight='bold')\n\naxes[1].pie(target_counts.values, labels=['No Disease (0)', 'Disease (1)'], #grafico de pizza com os valores normalizados\n            autopct='%1.1f%%', startangle=90, colors=['#2ecc71', '#e74c3c'])\naxes[1].set_title('Heart Disease Distribution (%)', fontsize=14, fontweight='bold')\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2. Correlação entre features numéricas e target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "numeric_data = x_train[numeric_features].copy()\nif y_train.dtype == 'object': #convert para dados numericas doneças ou sem doenças\n    y_numeric = y_train.map({'Absence': 0, 'Presence': 1}).values #convert 0 e 1\n    print(f\"Unique values: {y_train.unique()} -> {np.unique(y_numeric)}\")\nelse:\n    y_numeric = y_train\n\n\nnumeric_data['Heart Disease'] = y_numeric\ncorrelation_matrix = numeric_data.corr() #matriz de correlação\ntarget_corr = correlation_matrix['Heart Disease'].drop('Heart Disease').sort_values(ascending=False)\nfig, axes = plt.subplots(1, 2, figsize=(16, 6))\nsns.heatmap(correlation_matrix, annot=True, fmt='.3f', cmap='coolwarm', center=0, \n            ax=axes[0], linewidths=0.5, cbar_kws={'label': 'Correlation'})\naxes[0].set_title('Correlation Matrix (Numeric Features + Target)', fontsize=14, fontweight='bold')\n\n\ntarget_corr.plot(kind='barh', ax=axes[1], color=['green' if x > 0 else 'red' for x in target_corr]) #para saber quas influenciam em doenças\naxes[1].set_title('Feature Correlation with Heart Disease', fontsize=14, fontweight='bold')\naxes[1].set_xlabel('Correlation Coefficient')\naxes[1].axvline(0, color='black', linewidth=0.8)\n\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Categorical Features vs Target Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "n_categorical = len(categorical_features) #quantidade de variaveis categorias\nn_cols = 3\nn_rows = (n_categorical + n_cols - 1) // n_cols #quantidade de linha para plots , 3 linhas de graficos\nfig, axes = plt.subplots(n_rows, n_cols, figsize=(18, n_rows * 4)) #subplot 3x3\naxes = axes.flatten()\n\nfor idx, feature in enumerate(categorical_features):\n    ax = axes[idx]\n    cross_tab = pd.crosstab(x_train[feature], y_train, normalize='index') * 100 #Cruza os dados para contar quantos pacientes existem em cada combinação\n    #Transforma os números brutos em proporções dentro de cada categoria. \n    cross_tab.plot(kind='bar', stacked=True, ax=ax, color=['#2ecc71', '#e74c3c'],  #plot das barras\n                   edgecolor='black', legend=False)\n    ax.set_title(f'{feature} vs Heart Disease', fontweight='bold')#titulo grafico\n    ax.set_xlabel(feature)\n    ax.set_ylabel('Percentage (%)')\n    ax.set_xticklabels(ax.get_xticklabels(), rotation=45, ha='right')\n    ax.grid(alpha=0.3, axis='y')\n\nfor idx in range(n_categorical, len(axes)):\n    axes[idx].axis('off')\n\nhandles = [plt.Rectangle((0,0),1,1, color='#2ecc71'), plt.Rectangle((0,0),1,1, color='#e74c3c')]#cores legenda\nlabels = ['No Disease (0)', 'Disease (1)'] #labels leganda\nfig.legend(handles, labels, loc='upper right', fontsize=12)\n\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Análise das Variáveis Categóricas (Incidência de Doença)\n",
    "\n",
    "* **Sex:** Apresenta uma disparidade significativa; o grupo masculino (1) demonstra uma taxa de incidência de doença consideravelmente superior ao grupo feminino (0), sendo um forte divisor demográfico no modelo.\n",
    "* **Chest pain type:** O tipo 4 (Assintomática) é o indicador mais crítico, apresentando as maiores taxas de presença da doença, enquanto o tipo 2 (Angina atípica) apresenta a menor incidência proporcional.\n",
    "* **FBS over 120:** Embora indique risco metabólico, a taxa de doença entre quem tem glicemia alta (>120) é muito similar à de quem tem glicemia normal, sugerindo ser o preditor categórico de menor impacto individual.\n",
    "* **EKG results:** Pacientes com resultados tipo 2 (Hipertrofia ventricular esquerda) mostram uma probabilidade de doença cardíaca visivelmente maior em comparação aos que apresentam resultados normais (0).\n",
    "* **Exercise angina:** Um dos preditores mais binários; a presença de angina induzida por exercício (1) eleva drasticamente a taxa de doença cardíaca positiva em comparação a quem não manifesta o sintoma (0).\n",
    "* **Slope of ST:** A inclinação descendente ou plana no teste de esforço está fortemente correlacionada à presença da doença, enquanto a inclinação ascendente é um forte indicador de ausência (saúde cardíaca).\n",
    "* **Thallium:** Variável de alta especificidade; o resultado \"Reversível\" apresenta uma taxa de doença muito superior ao resultado \"Normal\", tornando-se um dos filtros mais precisos para o diagnóstico final."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. DATA PREPROCESSING\n",
    "\n",
    "---------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Categorical Variables Transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "y_train_transformed = y_train.map({'Absence': 0, 'Presence': 1}).values #absence tera valor 0 e presença valor\nprint(f\"\\nTarget shape: {y_train_transformed.shape}\")\nprint(f\"Unique values: {np.unique(y_train_transformed)}\")\nprint(f\"Distribution: {np.bincount(y_train_transformed)}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Feature Engineering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "from sklearn.linear_model import LogisticRegression\n\nscaler_lr = StandardScaler()# Treinar regressão logística simples\nX_scaled = scaler_lr.fit_transform(x_train[numeric_features]) #normaliza dados numerricos\nlr = LogisticRegression(random_state=42)\nlr.fit(X_scaled, y_train_transformed) #treina modelo\n\n# Extrair coeficientes para usar na criaçao e features\ncoefficients = dict(zip(numeric_features, lr.coef_[0]))\nprint(\"Coeficientes aprendidos:\")\nfor feat, coef in coefficients.items():\n    print(f\"  {feat}: {coef:.4f}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_features(df):\n",
    "    df = df.copy()\n",
    "    eps = 1e-6\n",
    "    \n",
    "    # 1. Polynomial features (Square)\n",
    "    df[\"Age_sq\"] = df[\"Age\"] ** 2\n",
    "    df[\"Cholesterol_sq\"] = df[\"Cholesterol\"] ** 2\n",
    "    df[\"Max HR_sq\"] = df[\"Max HR\"] ** 2\n",
    "    df[\"ST depression_sq\"] = df[\"ST depression\"] ** 2\n",
    "    \n",
    "    # 2. Mathematical transformations (Log/Sqrt)\n",
    "    df[\"log_age\"] = np.log1p(df[\"Age\"])\n",
    "    df[\"log_cholesterol\"] = np.log1p(df[\"Cholesterol\"])\n",
    "    df[\"log_st_depression\"] = np.log1p(df[\"ST depression\"])\n",
    "    df[\"sqrt_age\"] = np.sqrt(df[\"Age\"])\n",
    "    df[\"sqrt_cholesterol\"] = np.sqrt(df[\"Cholesterol\"])\n",
    "    df[\"sqrt_maxhr\"] = np.sqrt(df[\"Max HR\"])\n",
    "    \n",
    "    # 3. Interactions (Multiplications)\n",
    "    df[\"age_x_st_depression\"] = df[\"Age\"] * df[\"ST depression\"]\n",
    "    df[\"maxhr_x_st_depression\"] = df[\"Max HR\"] * df[\"ST depression\"]\n",
    "    df[\"age_x_maxhr\"] = df[\"Age\"] * df[\"Max HR\"]\n",
    "    df[\"age_x_cholesterol\"] = df[\"Age\"] * df[\"Cholesterol\"]\n",
    "    df[\"cholesterol_x_st_depression\"] = df[\"Cholesterol\"] * df[\"ST depression\"]\n",
    "\n",
    "    # 4. Ratios (Divisions)\n",
    "    df[\"cholesterol_per_age\"] = df[\"Cholesterol\"] / (df[\"Age\"] + eps)\n",
    "    df[\"maxhr_per_age\"] = df[\"Max HR\"] / (df[\"Age\"] + eps)\n",
    " \n",
    "    # 5. Differences (Deviation from mean)\n",
    "    df[\"age_diff_mean\"] = df[\"Age\"] - df[\"Age\"].mean()\n",
    "    df[\"cholesterol_diff_mean\"] = df[\"Cholesterol\"] - df[\"Cholesterol\"].mean()\n",
    "    df[\"maxhr_diff_mean\"] = df[\"Max HR\"] - df[\"Max HR\"].mean()\n",
    "    \n",
    "    # 6. Binning (Discretization)\n",
    "    df[\"age_bin\"] = pd.cut(df[\"Age\"], bins=[-1, 40, 50, 60, 100], labels=False)\n",
    "    df[\"cholesterol_bin\"] = pd.cut(df[\"Cholesterol\"], bins=[-1, 200, 240, 280, 600], labels=False)\n",
    "    df[\"maxhr_bin\"] = pd.cut(df[\"Max HR\"], bins=[-1, 100, 130, 160, 250], labels=False)\n",
    "    df[\"st_bin\"] = pd.cut(df[\"ST depression\"], bins=[-1, 0.5, 1.5, 3.0, 10], labels=False)\n",
    "    \n",
    "    # 7. Boolean Flags (Risk factors)\n",
    "    df[\"elderly\"] = (df[\"Age\"] >= 60).astype(int)\n",
    "    df[\"high_cholesterol\"] = (df[\"Cholesterol\"] >= 240).astype(int)\n",
    "    df[\"high_st_depression\"] = (df[\"ST depression\"] > 2.0).astype(int)\n",
    "    \n",
    "    # 8. Risk Scores\n",
    "    df[\"learned_risk_score\"] = (\n",
    "        df[\"Age\"] * 0.4244 +\n",
    "        df[\"BP\"] * (-0.0044) +\n",
    "        df[\"Cholesterol\"] * 0.1441 +\n",
    "        df[\"Max HR\"] * (-1.0234) +\n",
    "        df[\"ST depression\"] * 0.9945\n",
    "    )\n",
    "    \n",
    "    df[\"risk_factors_count\"] = (\n",
    "        df[\"elderly\"] + \n",
    "        df[\"high_cholesterol\"] + \n",
    "        df[\"high_st_depression\"]\n",
    "    )\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train_features = create_features(x_train)\n",
    "X_test_features  = create_features(X_test)\n",
    "print(f\"Features após create_features: {x_train_features.shape[1]}\")\n",
    "\n",
    "# --- 9. Frequency Encoding ---\n",
    "all_data = pd.concat([x_train_features, X_test_features])\n",
    "all_features = numeric_features + categorical_features\n",
    "for col in all_features:\n",
    "    freq = all_data[col].value_counts(normalize=True)\n",
    "    x_train_features[col + \"_freq\"] = x_train_features[col].map(freq)\n",
    "    X_test_features[col + \"_freq\"] = X_test_features[col].map(freq)\n",
    "print(f\"Features após Frequency Encoding: {x_train_features.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.3 Feature Quality Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Correlation with target\ndf_corr = x_train_features.copy()\ndf_corr['target'] = y_train_transformed\ncorr_target = df_corr.corr(numeric_only=True)['target'].drop('target') #calcula correlação com o target\ncorr_sorted = corr_target.abs().sort_values(ascending=False)\n\n#Visualization Features\nheight = 12\nplt.figure(figsize=(14, height))\n\ntop_corr = corr_target.loc[corr_sorted.head(42).index]\ncolors = ['green' if x > 0 else 'red' for x in top_corr.values]\n\nbars = plt.barh(range(len(top_corr)), top_corr.values, color=colors)\nplt.yticks(range(len(top_corr)), top_corr.index)\nplt.xlabel('Correlation Coefficient', fontsize=12)\nplt.title('Top Features - Correlation with Heart Disease', fontsize=14, fontweight='bold')\nplt.axvline(0, color='black', linewidth=0.8)\nplt.grid(alpha=0.3, axis='x')\n\nfor i, (feat, val) in enumerate(zip(top_corr.index, top_corr.values)):\n    plt.text(val + 0.01 if val > 0 else val - 0.01, i, f'{val:.3f}', \n             va='center', ha='left' if val > 0 else 'right', fontsize=9)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.4 Normalização (StandardScaler)\n",
    "\n",
    "MLP é sensível à escala dos dados. Diferente de árvores (XGBoost, LightGBM), redes neurais precisam que todas as features estejam na mesma escala para convergir corretamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(x_train_features)\n",
    "X_test_scaled = scaler.transform(X_test_features)\n",
    "\n",
    "print(f\"X_train_scaled shape: {X_train_scaled.shape}\")\n",
    "print(f\"X_test_scaled shape: {X_test_scaled.shape}\")\n",
    "print(f\"\\nMean (primeiras 5 features): {X_train_scaled[:, :5].mean(axis=0).round(4)}\")\n",
    "print(f\"Std  (primeiras 5 features): {X_train_scaled[:, :5].std(axis=0).round(4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.5 Model Evaluation Function (Cross-Validation)\n",
    "\n",
    "Adaptada para MLP - usa dados já normalizados e não usa `clone()` (MLP precisa de recriação manual para reset de pesos)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_mlp_cv(model_params, X, y, X_test=None, cv=5):\n",
    "    \"\"\"\n",
    "    Avalia MLP com cross-validation.\n",
    "    Recebe params dict ao invés de modelo, pois MLP precisa ser recriado a cada fold\n",
    "    para resetar os pesos da rede neural.\n",
    "    \"\"\"\n",
    "    skf = StratifiedKFold(n_splits=cv, shuffle=True, random_state=42)\n",
    "    roc_auc_scores = []\n",
    "    oof_preds = np.zeros(len(y))\n",
    "    \n",
    "    if X_test is not None:\n",
    "        test_preds_folds = np.zeros((len(X_test), cv))\n",
    "    \n",
    "    for fold, (train_idx, val_idx) in enumerate(skf.split(X, y)):\n",
    "        X_train_fold = X[train_idx]\n",
    "        X_val_fold = X[val_idx]\n",
    "        y_train_fold = y[train_idx]\n",
    "        y_val_fold = y[val_idx]\n",
    "        \n",
    "        # Cria novo MLP a cada fold (pesos resetados)\n",
    "        model_fold = MLPClassifier(**model_params)\n",
    "        model_fold.fit(X_train_fold, y_train_fold)\n",
    "        \n",
    "        oof_preds[val_idx] = model_fold.predict_proba(X_val_fold)[:, 1]\n",
    "        \n",
    "        if X_test is not None:\n",
    "            test_preds_folds[:, fold] = model_fold.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        roc_auc = roc_auc_score(y_val_fold, oof_preds[val_idx])\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "    \n",
    "    if X_test is not None:\n",
    "        test_preds = test_preds_folds.mean(axis=1)\n",
    "        return np.array(roc_auc_scores), oof_preds, test_preds\n",
    "    \n",
    "    return np.array(roc_auc_scores), oof_preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. MLP - Otimização com Optuna\n",
    "\n",
    "----------\n",
    "\n",
    "O Optuna vai buscar a melhor arquitetura de rede neural:\n",
    "- Número de camadas ocultas (1 a 3)\n",
    "- Número de neurônios por camada\n",
    "- Learning rate\n",
    "- Regularização (alpha)\n",
    "- Função de ativação\n",
    "- Batch size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective_mlp(trial):\n",
    "    # Arquitetura da rede\n",
    "    n_layers = trial.suggest_int('n_layers', 1, 3)\n",
    "    layers = []\n",
    "    for i in range(n_layers):\n",
    "        n_units = trial.suggest_int(f'n_units_l{i}', 64, 512)\n",
    "        layers.append(n_units)\n",
    "    \n",
    "    params = {\n",
    "        'hidden_layer_sizes': tuple(layers),\n",
    "        'activation': trial.suggest_categorical('activation', ['relu', 'tanh']),\n",
    "        'learning_rate_init': trial.suggest_float('learning_rate_init', 1e-4, 1e-2, log=True),\n",
    "        'alpha': trial.suggest_float('alpha', 1e-6, 1e-1, log=True),\n",
    "        'batch_size': trial.suggest_categorical('batch_size', [256, 512, 1024, 2048]),\n",
    "        'max_iter': 300,\n",
    "        'early_stopping': True,\n",
    "        'validation_fraction': 0.1,\n",
    "        'n_iter_no_change': 15,\n",
    "        'random_state': 42,\n",
    "        'solver': 'adam',\n",
    "    }\n",
    "    \n",
    "    # CV com 3 folds para ser mais rápido na busca\n",
    "    skf = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "    scores = []\n",
    "    \n",
    "    for train_idx, val_idx in skf.split(X_train_scaled, y_train_transformed):\n",
    "        X_tr = X_train_scaled[train_idx]\n",
    "        X_val = X_train_scaled[val_idx]\n",
    "        y_tr = y_train_transformed[train_idx]\n",
    "        y_val = y_train_transformed[val_idx]\n",
    "        \n",
    "        model = MLPClassifier(**params)\n",
    "        model.fit(X_tr, y_tr)\n",
    "        \n",
    "        y_pred = model.predict_proba(X_val)[:, 1]\n",
    "        scores.append(roc_auc_score(y_val, y_pred))\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Rodar Optuna\n",
    "sampler = TPESampler(seed=42)\n",
    "study_mlp = optuna.create_study(direction='maximize', sampler=sampler)\n",
    "study_mlp.optimize(objective_mlp, n_trials=50, show_progress_bar=True)\n",
    "\n",
    "print(f\"\\nMelhor ROC-AUC: {study_mlp.best_value:.6f}\")\n",
    "print(f\"Melhores parâmetros:\")\n",
    "for key, value in study_mlp.best_params.items():\n",
    "    print(f\"  {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.1 Visualização do Optuna"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Histórico de trials\n",
    "fig, axes = plt.subplots(1, 2, figsize=(16, 5))\n",
    "\n",
    "# Optimization history\n",
    "trials = study_mlp.trials\n",
    "values = [t.value for t in trials if t.value is not None]\n",
    "axes[0].plot(values, 'o-', alpha=0.6, markersize=4)\n",
    "axes[0].axhline(y=study_mlp.best_value, color='r', linestyle='--', label=f'Best: {study_mlp.best_value:.6f}')\n",
    "axes[0].set_xlabel('Trial')\n",
    "axes[0].set_ylabel('ROC-AUC')\n",
    "axes[0].set_title('Optuna MLP - Optimization History', fontweight='bold')\n",
    "axes[0].legend()\n",
    "axes[0].grid(alpha=0.3)\n",
    "\n",
    "# Importância dos hiperparâmetros\n",
    "importances = optuna.importance.get_param_importances(study_mlp)\n",
    "params_names = list(importances.keys())[:10]\n",
    "params_values = list(importances.values())[:10]\n",
    "axes[1].barh(params_names, params_values, color='steelblue')\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Hyperparameter Importance', fontweight='bold')\n",
    "axes[1].grid(alpha=0.3, axis='x')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 MLP com melhores parâmetros do Optuna\n",
    "\n",
    "----------\n",
    "\n",
    "Agora treina o MLP final com os melhores parâmetros encontrados, usando 5-fold CV completo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reconstruir hidden_layer_sizes a partir dos best_params\n",
    "best = study_mlp.best_params\n",
    "n_layers = best['n_layers']\n",
    "layers = tuple(best[f'n_units_l{i}'] for i in range(n_layers))\n",
    "\n",
    "best_mlp_params = {\n",
    "    'hidden_layer_sizes': layers,\n",
    "    'activation': best['activation'],\n",
    "    'learning_rate_init': best['learning_rate_init'],\n",
    "    'alpha': best['alpha'],\n",
    "    'batch_size': best['batch_size'],\n",
    "    'max_iter': 500,\n",
    "    'early_stopping': True,\n",
    "    'validation_fraction': 0.1,\n",
    "    'n_iter_no_change': 20,\n",
    "    'random_state': 42,\n",
    "    'solver': 'adam',\n",
    "}\n",
    "\n",
    "print(f\"Arquitetura MLP: {layers}\")\n",
    "print(f\"Activation: {best['activation']}\")\n",
    "print(f\"Learning rate: {best['learning_rate_init']:.6f}\")\n",
    "print(f\"Alpha: {best['alpha']:.6f}\")\n",
    "print(f\"Batch size: {best['batch_size']}\")\n",
    "\n",
    "roc_scores_mlp, oof_mlp, y_test_proba_mlp = evaluate_mlp_cv(\n",
    "    best_mlp_params, X_train_scaled, y_train_transformed, X_test_scaled, cv=5\n",
    ")\n",
    "\n",
    "for fold, score in enumerate(roc_scores_mlp, 1):\n",
    "    print(f\"  Fold {fold}: {score:.4f}\")\n",
    "print(f\"\\nMean ROC-AUC: {roc_scores_mlp.mean():.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "submission_mlp = pd.DataFrame({\n",
    "    'id': test['id'],\n",
    "    'Heart Disease': y_test_proba_mlp\n",
    "})\n",
    "\n",
    "submission_mlp.to_csv('submission_MLP.csv', index=False)\n",
    "print(\"MLP submission salvo: submission_MLP.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Salvar parâmetros otimizados\n",
    "\n",
    "Salva os melhores parâmetros para uso futuro no ensemble com os modelos boost."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "\n",
    "os.makedirs('Optuna_results', exist_ok=True)\n",
    "\n",
    "# Salvar parâmetros\n",
    "mlp_results = {\n",
    "    'best_params': {\n",
    "        'hidden_layer_sizes': list(layers),\n",
    "        'activation': best['activation'],\n",
    "        'learning_rate_init': best['learning_rate_init'],\n",
    "        'alpha': best['alpha'],\n",
    "        'batch_size': best['batch_size'],\n",
    "    },\n",
    "    'best_roc_auc_optuna': study_mlp.best_value,\n",
    "    'mean_roc_auc_5fold': float(roc_scores_mlp.mean()),\n",
    "    'fold_scores': roc_scores_mlp.tolist(),\n",
    "    'n_trials': len(study_mlp.trials),\n",
    "}\n",
    "\n",
    "with open('Optuna_results/mlp_best_params.json', 'w') as f:\n",
    "    json.dump(mlp_results, f, indent=2)\n",
    "\n",
    "print(\"Resultados salvos em Optuna_results/mlp_best_params.json\")\n",
    "print(f\"\\nResumo:\")\n",
    "print(f\"  Arquitetura: {layers}\")\n",
    "print(f\"  ROC-AUC (Optuna 3-fold): {study_mlp.best_value:.6f}\")\n",
    "print(f\"  ROC-AUC (Final 5-fold):  {roc_scores_mlp.mean():.6f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}